{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чтобы установить ipywidgets:\n",
    "\n",
    "`!pip install ipywidgets`\n",
    "\n",
    "__Для тех кто на Jupyter Notebook:__\n",
    "\n",
    "`!jupyter nbextension enable --py widgetsnbextension`\n",
    "\n",
    "__Для тех, кто на Jupyter Lab:__\n",
    "\n",
    "`!jupyter labextension install @jupyter-widgets/jupyterlab-manager`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# в моем случае это Jupyter Lab, поэтому я прямо из ноутбука могу доустановить нужный модель\n",
    "\n",
    "# прежде виджет нужно скачать\n",
    "!pip install ipywidgets\n",
    "\n",
    "# И далее установить виджет в jupyter lab и активировать его\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# По материалам <u>[Neural Network from scratch](https://towardsdatascience.com/neural-networks-from-scratch-easy-vs-hard-b26ddc2e89c7)</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# функция активации нейорна\n",
    "def sigmoid(s):    \n",
    "    return 1/(1 + np.exp(-s))\n",
    "\n",
    "# Производная сигмоиды для метода обратного распространения ошибки (backpropagation)\n",
    "def sigmoid_deriv(s):\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Функция активации выходного слоя Softmax. \n",
    "# Первое что гугл выдал: https://www.machinelearningmastery.ru/the-softmax-function-neural-net-outputs-as-probabilities-and-ensemble-classifiers-9bd94d75932/\n",
    "# Softmax формирует интерпретируемость выходного слоя как набор вероятностей отнесения каждого выходного нейрона как\n",
    "# вероятности отнесения входного вектора к какой-то одной из категорий (при условии, что мы используем softmax как функцию активации выходного слоя и кросс-энтропию для функции потерь.\n",
    "# Сумма всех вероятностей выходного слоя равна единице.\n",
    "# Так например, если предсказываем ирисы, то у нас есть три категории. И тогда у нас будет три нейрона на выходном слое.\n",
    "# Каждый нейрон показывает вероятность отнесения входного вектора к своей категории. \n",
    "# Например выходной вектор с функцией активации softmax выдаёт такой вектор [0.31, 0.52, 0.17]\n",
    "# Его можно интерпретировать следующим образом: второй нейрон (средний) имеет самую большую вероятность, \n",
    "# значит предполагаем, что это вторая категория.\n",
    "def softmax(s):\n",
    "    exps = np.exp(s - np.max(s, axis=1, keepdims=True))\n",
    "    return exps/np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def cross_entropy(pred, real):\n",
    "    n_samples = real.shape[0]\n",
    "    residuals = pred - real\n",
    "    return residuals/n_samples\n",
    "\n",
    "\n",
    "def error(pred, real):\n",
    "    n_samples = real.shape[0]\n",
    "    logp = -np.log(pred[np.arange(n_samples), real.argmax(axis=1)])\n",
    "    loss = np.sum(logp)/n_samples\n",
    "    return loss\n",
    "\n",
    "# Эта та же самая Accuracy, что и в задаче с Титаником\n",
    "def get_accuracy(model, X, y):\n",
    "    accuracy = 0\n",
    "    for xx, yy in zip(X,y):\n",
    "        prediction = model.predict(xx)\n",
    "        if prediction == np.argmax(yy):\n",
    "            accuracy += 1\n",
    "    return accuracy/len(X)\n",
    "\n",
    "\n",
    "class NN:\n",
    "    # Инициализируем экземпляр класса\n",
    "    def __init__(self, X, y, neurons = 128, lr = 0.5):\n",
    "        self.X = X\n",
    "        self.y = y        \n",
    "        self.lr = lr\n",
    "        ip_dim = X.shape[1] #input layer\n",
    "        op_dim = y.shape[1] #output layer\n",
    "        \n",
    "        self.w1 = np.random.randn(ip_dim, neurons) #weights\n",
    "        self.b1 = np.zeros((1, neurons)) #biases\n",
    "        \n",
    "        self.w2 = np.random.randn(neurons, neurons)\n",
    "        self.b2 = np.zeros((1, neurons))\n",
    "        \n",
    "        self.w3 = np.random.randn(neurons, op_dim)\n",
    "        self.b3 = np.zeros((1, op_dim))\n",
    "\n",
    "\n",
    "    def feedforward(self):\n",
    "        z1 = np.dot(self.X, self.w1) + self.b1\n",
    "        self.tmp_z1 = z1\n",
    "        self.a1 = sigmoid(z1)\n",
    "\n",
    "        z2 = np.dot(self.a1, self.w2) + self.b2\n",
    "        self.tmp_z2 = z2\n",
    "        self.a2 = sigmoid(z2)\n",
    "\n",
    "        z3 = np.dot(self.a2, self.w3) + self.b3\n",
    "        self.tmp_z3 = z3\n",
    "        self.a3 = softmax(z3) \n",
    "    \n",
    "\n",
    "    def get_loss(self):\n",
    "        loss = error(self.a3, self.y)\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def backprop(self):\n",
    "        a3_delta = cross_entropy(self.a3, self.y) #weights3\n",
    "        z2_delta = np.dot(a3_delta, self.w3.T)\n",
    "        \n",
    "        a2_delta = z2_delta * sigmoid_deriv(self.a2) #weights2\n",
    "        z1_delta = np.dot(a2_delta, self.w2.T)\n",
    "        \n",
    "        a1_delta = z1_delta * sigmoid_deriv(self.a1) #weights1\n",
    "        \n",
    "        self.w3 -= self.lr * np.dot(self.a2.T, a3_delta)\n",
    "        self.b3 -= self.lr * np.sum(a3_delta, axis=0, keepdims=True)\n",
    "        self.w2 -= self.lr * np.dot(self.a1.T, a2_delta)\n",
    "        self.b2 -= self.lr * np.sum(a2_delta, axis=0)\n",
    "        self.w1 -= self.lr * np.dot(self.X.T, a1_delta)\n",
    "        self.b1 -= self.lr * np.sum(a1_delta, axis=0)\n",
    "        \n",
    "        \n",
    "    def predict(self, data):\n",
    "        self.X = data\n",
    "        self.feedforward()\n",
    "        return self.a3.argmax()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d5e401d524452280371976ebfc1954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:     96.91%\n",
      " Test accuracy:     82.22%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import HTML\n",
    "import time\n",
    "\n",
    "dig = load_digits() # получаем датасет\n",
    "onehot_target = pd.get_dummies(dig.target)\n",
    "dig.keys()\n",
    "X_train, X_test, y_train, y_test = train_test_split(dig.data, onehot_target, \n",
    "                                                    test_size = 0.1, random_state=42)\n",
    "\n",
    "nn = NN(X_train, np.array(y_train), neurons = 128, lr = 0.2)\n",
    "\n",
    "epochs = 300\n",
    "\n",
    "h = HTML(); display(h)\n",
    "previous_loss = 0\n",
    "previous_validation_loss = 100\n",
    "ts = time.time()\n",
    "for epoch in range(epochs):\n",
    "    nn.feedforward() # прямой прогон модели\n",
    "    loss = nn.get_loss()\n",
    "    # проверка, что если модель начала\n",
    "    # У нас есть X_test, y_test, но по ним будем проверять финальное качество модели. \n",
    "    # Однако ничто не мешает обучающую выборку разбить еще раз на обучающую\\валидационную и проверять\n",
    "    # модель на переобучение\\необходимость сделать остановку в обуччении\n",
    "    # validation_loss = nn.get_loss(X_validation, y_validation) \n",
    "    h.value = '''\n",
    "        Error on {epoch} epoch: {loss:.2%} (change: {change:>8.6%})<br>\n",
    "        Time passed: {time:.1f}s'''.format(\n",
    "        epoch = epoch, loss = loss, change = loss - previous_loss, \n",
    "        time=time.time()-ts)\\\n",
    "        .replace(' ', '&nbsp;')    \n",
    "    nn.backprop() # обновляем веса\n",
    "    \n",
    "    previous_loss = loss\n",
    "        \n",
    "print(\"Train accuracy: {0:>10.2%}\".format(get_accuracy(nn, X_train, np.array(y_train))))\n",
    "print(\" Test accuracy: {0:>10.2%}\".format(get_accuracy(nn, X_test, np.array(y_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Пару абзацев про датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# В качестве датасета используется \n",
    "dig = load_digits() # загружаем один из датасетов через sklearn\n",
    "print(type(dig)) # Это не DICT\n",
    "dig.keys() #Но ведет себя почти как dict\n",
    "dig['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 5620\n",
      "    :Number of Attributes: 64\n",
      "    :Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      "    :Missing Attribute Values: None\n",
      "    :Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      "    :Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  - C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "    Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "    Graduate Studies in Science and Engineering, Bogazici University.\n",
      "  - E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "  - Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "    Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "    Electrical and Electronic Engineering Nanyang Technological University.\n",
      "    2005.\n",
      "  - Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "    Algorithm. NIPS. 2000.\n"
     ]
    }
   ],
   "source": [
    "print(dig['DESCR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " dig['data'][0] # один из подаваемых векторов (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.],\n",
       "       [ 0.,  0., 13., 15., 10., 15.,  5.,  0.],\n",
       "       [ 0.,  3., 15.,  2.,  0., 11.,  8.,  0.],\n",
       "       [ 0.,  4., 12.,  0.,  0.,  8.,  8.,  0.],\n",
       "       [ 0.,  5.,  8.,  0.,  0.,  9.,  8.,  0.],\n",
       "       [ 0.,  4., 11.,  0.,  1., 12.,  7.,  0.],\n",
       "       [ 0.,  2., 14.,  5., 10., 12.,  0.,  0.],\n",
       "       [ 0.,  0.,  6., 13., 10.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dig['images'][0] # эти же данные, но уже в удобном для отрисовки в human-readable формате"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f592c276d0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Так выглядит один из подаваемых экземпляров\n",
    "plt.imshow(dig['data'][50].reshape(8,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Вот так выглядит входной вектор\n",
    "plt.imshow(dig['data'][50].reshape(64,1))\n",
    "plt.show()\n",
    "# или так\n",
    "plt.imshow(dig['data'][50].reshape(1,64))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "На самостоятельное изучение:\n",
    "\n",
    "1) Поиграться с архитектурой сети:\n",
    "    + Добавить\\Удалить скрытые слои (обновить методы feedforward, backprop, __init__)\n",
    "    + Посмотреть как зависит переобучение модели от learning rate (lr)\n",
    "    + Посмотреть как зависит переобучение модели от количества эпох\n",
    "    + Заменить метод активации нейрона с sigmoid на ReLU\n",
    "    + На каждой эпохе валидировать модель на отложенной выборке\n",
    "    + Если оценка валидации перестала изменяться (или начала ухудшаться) - прекратить обучение сети\n",
    "    + ?Визуализировать вектор изменения первого скрытого слоя (см. пример с разворотом входящего вектора в столбец)\n",
    "\n",
    "2) Подменить датасет на iris\n",
    "    + На ирисе входной вектор из 4 элементов - сделать архитектуру 1 скрытый слой и 4 нейрона\n",
    "    + Инициализировать экземпляр NN, обучить и после забрать из модели weights\\biases и ручками в эксельке посчитать\n",
    "3) Подменить датасет на титаник\n",
    "4) Построить confusion_maxtrix, classification_report\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width species\n",
       "0           5.1          3.5           1.4          0.2  setosa\n",
       "1           4.9          3.0           1.4          0.2  setosa\n",
       "2           4.7          3.2           1.3          0.2  setosa\n",
       "3           4.6          3.1           1.5          0.2  setosa\n",
       "4           5.0          3.6           1.4          0.2  setosa"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Датасет для ириса\n",
    "iris = pd.read_csv('../../data/raw/iris.csv')\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WTR:\n",
    "+ ## <u>[Neural Network from scratch](https://towardsdatascience.com/neural-networks-from-scratch-easy-vs-hard-b26ddc2e89c7)</u>\n",
    "\n",
    "+ ## <u>[Michael Nielsen __Neural Networks and Deep Learning__](http://neuralnetworksanddeeplearning.com/index.html)</u>\n",
    "https://medium.com/@pdquant/all-the-backpropagation-derivatives-d5275f727f60\n",
    "Функции активации https://towardsdatascience.com/comparison-of-activation-functions-for-deep-neural-networks-706ac4284c8a"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
